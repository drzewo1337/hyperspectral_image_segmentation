{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperspectral Images Segmentation\n",
        "\n",
        "## Datasets:\n",
        "- Indian Pines: 145×145, 200 wavelenghts, 16 classes\n",
        "- PaviaU: 610×340, 103 wavelenghts, 9 classes\n",
        "- PaviaC: 1096×715, 102 wavelenghts, 9 classes\n",
        "- KSC: 512×614, 176 wavelenghts, 13 classes\n",
        "- Salinas: 512×217, 204 wavelenghts, 16 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import ssl\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler \n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "try:\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "    requests.packages.urllib3.disable_warnings()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Parametry\n",
        "patch_size = 16\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "val_split = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: dataset configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET_URLS = {\n",
        "    'Indian': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat',\n",
        "    },\n",
        "    'PaviaU': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat',\n",
        "    },\n",
        "    'PaviaC': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/f/f0/Pavia.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/5/53/Pavia_gt.mat',\n",
        "    },\n",
        "    'KSC': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/2/26/KSC.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/a/a6/KSC_gt.mat',\n",
        "    },\n",
        "    'Salinas': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/a/a3/Salinas_corrected.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/f/fa/Salinas_gt.mat',\n",
        "    }\n",
        "}\n",
        "\n",
        "DATASET_KEYS = {\n",
        "    'Indian': {\n",
        "        'data': ['indian_pines_corrected', 'Indian_pines_corrected'],\n",
        "        'gt': ['indian_pines_gt', 'Indian_pines_gt']\n",
        "    },\n",
        "    'PaviaU': {\n",
        "        'data': ['paviaU', 'PaviaU', 'pavia_u'],\n",
        "        'gt': ['paviaU_gt', 'PaviaU_gt', 'pavia_u_gt']\n",
        "    },\n",
        "    'PaviaC': {\n",
        "        'data': ['pavia', 'Pavia', 'paviaC', 'PaviaC'],\n",
        "        'gt': ['pavia_gt', 'Pavia_gt', 'paviaC_gt', 'PaviaC_gt']\n",
        "    },\n",
        "    'KSC': {\n",
        "        'data': ['KSC', 'ksc'],\n",
        "        'gt': ['KSC_gt', 'ksc_gt']\n",
        "    },\n",
        "    'Salinas': {\n",
        "        'data': ['salinas_corrected', 'Salinas_corrected', 'salinas'],\n",
        "        'gt': ['salinas_gt', 'Salinas_gt']\n",
        "    }\n",
        "}\n",
        "\n",
        "DATASET_INFO = {\n",
        "    'Indian': {'num_classes': 16},\n",
        "    'PaviaU': {'num_classes': 9},\n",
        "    'PaviaC': {'num_classes': 9},\n",
        "    'KSC': {'num_classes': 13},\n",
        "    'Salinas': {'num_classes': 16}\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_file(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Pobieranie {filename}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(url, filename)\n",
        "            file_size = os.path.getsize(filename) / 1024 / 1024\n",
        "            print(f\"Pobrano {filename} ({file_size:.1f} MB)\")\n",
        "        except Exception as e:\n",
        "            try:\n",
        "                import requests\n",
        "                response = requests.get(url, verify=False, timeout=60)\n",
        "                response.raise_for_status()\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "                file_size = os.path.getsize(filename) / 1024 / 1024\n",
        "                print(f\"Pobrano {filename} ({file_size:.1f} MB) - użyto requests\")\n",
        "            except ImportError:\n",
        "                print(f\"Błąd: requests nie jest zainstalowane.\")\n",
        "                raise\n",
        "            except Exception as e2:\n",
        "                print(f\"Błąd pobierania {filename}: {str(e2)}\")\n",
        "                raise\n",
        "    else:\n",
        "        file_size = os.path.getsize(filename) / 1024 / 1024\n",
        "        print(f\"{filename} już istnieje ({file_size:.1f} MB)\")\n",
        "\n",
        "def find_key_in_mat(mat_file, possible_keys):\n",
        "    for key in possible_keys:\n",
        "        if key in mat_file:\n",
        "            return key\n",
        "\n",
        "    keys = [k for k in mat_file.keys() if not k.startswith('__')]\n",
        "    if keys:\n",
        "        return keys[0]\n",
        "    raise ValueError(f\"Nie znaleziono klucza w pliku .mat. Możliwe: {possible_keys}\")\n",
        "\n",
        "def load_dataset(dataset_name):\n",
        "    if dataset_name not in DATASET_URLS:\n",
        "        raise ValueError(f\"Unknown dataset: {dataset_name}. Available: {list(DATASET_URLS.keys())}\")\n",
        "    \n",
        "    urls = DATASET_URLS[dataset_name]\n",
        "    keys = DATASET_KEYS[dataset_name]\n",
        "    info = DATASET_INFO[dataset_name].copy()\n",
        "    \n",
        "    # Pobierz pliki\n",
        "    data_file = f\"{dataset_name}_data.mat\"\n",
        "    gt_file = f\"{dataset_name}_gt.mat\"\n",
        "    \n",
        "    download_file(urls['data'], data_file)\n",
        "    download_file(urls['gt'], gt_file)\n",
        "    \n",
        "    # Załaduj dane\n",
        "    mat_data = sio.loadmat(data_file)\n",
        "    mat_gt = sio.loadmat(gt_file)\n",
        "    \n",
        "    # Znajdź właściwe klucze\n",
        "    data_key = find_key_in_mat(mat_data, keys['data'])\n",
        "    gt_key = find_key_in_mat(mat_gt, keys['gt'])\n",
        "    \n",
        "    data = mat_data[data_key]\n",
        "    labels = mat_gt[gt_key]\n",
        "    \n",
        "    info['num_bands'] = data.shape[2] if len(data.shape) == 3 else data.shape[-1]\n",
        "    info['shape'] = data.shape\n",
        "    \n",
        "    print(f\"Załadowano {dataset_name}: shape={data.shape}, bands={info['num_bands']}, classes={info['num_classes']}\")\n",
        "    \n",
        "    return data, labels, info\n",
        "\n",
        "def normalize(data):\n",
        "    h, w, b = data.shape\n",
        "    data = data.reshape(-1, b)\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "    return data.reshape(h, w, b)\n",
        "\n",
        "def pad_with_zeros(data, margin):\n",
        "    return np.pad(data, ((margin, margin), (margin, margin), (0, 0)), mode='constant')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Załadowano modele\n"
          ]
        }
      ],
      "source": [
        "#model1\n",
        "class InceptionHSINet(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=16):\n",
        "        super(InceptionHSINet, self).__init__()\n",
        "        self.entry = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, 8, kernel_size=3, padding=1),\n",
        "            nn.Dropout3d(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=2)\n",
        "        )\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv3d(8, 16, kernel_size=1),\n",
        "            nn.Dropout3d(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.Dropout3d(0.3),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv3d(8, 16, kernel_size=3, padding=1),\n",
        "            nn.Dropout3d(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(16, 16, kernel_size=5, padding=2),\n",
        "            nn.Dropout3d(0.3),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv3d(8, 16, kernel_size=5, padding=2),\n",
        "            nn.Dropout3d(0.3),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.Dropout3d(0.3),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.pool = nn.AdaptiveAvgPool3d(1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16 * 3, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.entry(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        x = torch.cat([x1, x2, x3], dim=1)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "#model2\n",
        "class SimpleHSINet(nn.Module):\n",
        "    def __init__(self, input_channels=30, num_classes=16):\n",
        "        super(SimpleHSINet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, 90, kernel_size=1)\n",
        "        self.conv2 = nn.Conv2d(90, 270, kernel_size=3)\n",
        "        self.dropout1 = nn.Dropout2d(0.3)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(270, 180)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(180, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "#model3\n",
        "class CNNFromDiagram(nn.Module):\n",
        "    def __init__(self, input_channels=200, num_classes=16, patch_size=16):\n",
        "        super(CNNFromDiagram, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=100, kernel_size=3, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=100, out_channels=100, kernel_size=3, padding=0)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # Oblicz rozmiar wyjścia po konwolucjach\n",
        "        dummy_input = torch.zeros(1, input_channels, patch_size, patch_size)\n",
        "        x = self.pool1(F.relu(self.conv1(dummy_input)))\n",
        "        x = self.pool2(F.relu(self.conv2(x)))\n",
        "        flatten_dim = x.view(1, -1).shape[1]\n",
        "        \n",
        "        self.fc1 = nn.Linear(flatten_dim, 84)\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Załadowano modele\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: dataset classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HSI_Dataset(Dataset):\n",
        "    def __init__(self, dataset_name, patch_size=16, model_type='2d'):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.patch_size = patch_size\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        data, labels, self.info = load_dataset(dataset_name)\n",
        "        \n",
        "        # StandardScaler\n",
        "        data = normalize(data)\n",
        "        \n",
        "        # Padding\n",
        "        margin = patch_size // 2\n",
        "        padded_data = pad_with_zeros(data, margin)\n",
        "        \n",
        "        # patch extraction\n",
        "        h, w, _ = data.shape\n",
        "        self.patches = []\n",
        "        self.targets = []\n",
        "        \n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                label = labels[i, j]\n",
        "                if label == 0:\n",
        "                    continue\n",
        "                patch = padded_data[i:i+patch_size, j:j+patch_size, :]\n",
        "                self.patches.append(patch)\n",
        "                self.targets.append(label - 1) \n",
        "        \n",
        "        self.patches = np.array(self.patches)\n",
        "        self.targets = np.array(self.targets)\n",
        "        \n",
        "        \n",
        "        if model_type == '3d':\n",
        "            self.patches = np.transpose(self.patches, (0, 3, 1, 2))  # (N, B, H, W)\n",
        "            self.patches = np.expand_dims(self.patches, axis=1)  # (N, 1, B, H, W)\n",
        "        else:\n",
        "            self.patches = np.transpose(self.patches, (0, 3, 1, 2))  # (N, B, H, W)\n",
        "        \n",
        "        print(f\"Dataset {dataset_name}: {len(self)} samples, shape={self.patches.shape}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.patches)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.patches[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: trening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_loaders(dataset_name, patch_size=16, batch_size=64, val_split=0.2, model_type='2d'):\n",
        "    dataset = HSI_Dataset(dataset_name, patch_size, model_type)\n",
        "    val_len = int(len(dataset) * val_split)\n",
        "    train_len = len(dataset) - val_len\n",
        "    train_set, val_set = random_split(dataset, [train_len, val_len])\n",
        "    return DataLoader(train_set, batch_size=batch_size, shuffle=True), DataLoader(val_set, batch_size=batch_size), dataset.info\n",
        "\n",
        "def create_model(model_name, num_bands, num_classes, patch_size=16):\n",
        "    if model_name == 'InceptionHSINet':\n",
        "        # Model 3D - in_channels=1, num_classes\n",
        "        return InceptionHSINet(in_channels=1, num_classes=num_classes)\n",
        "    elif model_name == 'SimpleHSINet':\n",
        "        # Model 2D - input_channels=num_bands, num_classes\n",
        "        return SimpleHSINet(input_channels=num_bands, num_classes=num_classes)\n",
        "    elif model_name == 'CNNFromDiagram':\n",
        "        # Model 2D - input_channels=num_bands, num_classes, patch_size\n",
        "        return CNNFromDiagram(input_channels=num_bands, num_classes=num_classes, patch_size=patch_size)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=50, lr=0.001, device=None, \n",
        "          model_name=\"\", dataset_name=\"\"):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    log_file = f\"training_log_{model_name}_{dataset_name}.csv\"\n",
        "    \n",
        "    best_val_acc = 0.0\n",
        "    train_history = []\n",
        "    \n",
        "    with open(log_file, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Validation Accuracy\"])\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            total_loss, correct = 0, 0\n",
        "            \n",
        "            for inputs, labels in train_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            \n",
        "            train_acc = 100.0 * correct / len(train_loader.dataset)\n",
        "            \n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_correct = 0\n",
        "            with torch.no_grad():\n",
        "                for val_inputs, val_labels in val_loader:\n",
        "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
        "                    val_outputs = model(val_inputs)\n",
        "                    val_correct += (val_outputs.argmax(1) == val_labels).sum().item()\n",
        "            \n",
        "            val_acc = 100.0 * val_correct / len(val_loader.dataset)\n",
        "            \n",
        "            # Save results\n",
        "            writer.writerow([epoch + 1, total_loss, train_acc, val_acc])\n",
        "            train_history.append({\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': total_loss,\n",
        "                'train_acc': train_acc,\n",
        "                'val_acc': val_acc\n",
        "            })\n",
        "            \n",
        "            # Best model\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                torch.save(model.state_dict(), f\"best_model_{model_name}_{dataset_name}.pth\")\n",
        "            \n",
        "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "    \n",
        "    print(f\"Trening zakończony. Best Val Acc: {best_val_acc:.2f}%\")\n",
        "    return train_history, best_val_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: main loop for experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting 15 experiments: 3 models x 5 datasets\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT 1/15: InceptionHSINet on Indian\n",
            "================================================================================\n",
            "Indian_data.mat już istnieje (5.7 MB)\n",
            "Indian_gt.mat już istnieje (0.0 MB)\n",
            "Załadowano Indian: shape=(145, 145, 200), bands=200, classes=16\n",
            "Dataset Indian: 10249 samples, shape=(10249, 1, 200, 16, 16)\n",
            "Model: InceptionHSINet, Input: 200 bands, 16 classes\n",
            "Epoch 1/50, Loss: 299.6764, Train Acc: 23.84%, Val Acc: 35.29%\n",
            "Epoch 10/50, Loss: 214.0621, Train Acc: 42.12%, Val Acc: 54.12%\n",
            "Epoch 20/50, Loss: 194.5109, Train Acc: 47.62%, Val Acc: 57.00%\n",
            "Epoch 30/50, Loss: 178.6502, Train Acc: 50.66%, Val Acc: 62.27%\n"
          ]
        }
      ],
      "source": [
        "MODELS = ['InceptionHSINet', 'SimpleHSINet', 'CNNFromDiagram']\n",
        "DATASETS = ['Indian', 'PaviaU', 'PaviaC', 'KSC', 'Salinas']\n",
        "\n",
        "# dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Model type mapping (which models use 3D, which 2D)\n",
        "MODEL_TYPES = {\n",
        "    'InceptionHSINet': '3d',\n",
        "    'SimpleHSINet': '2d',\n",
        "    'CNNFromDiagram': '2d'\n",
        "}\n",
        "\n",
        "total_experiments = len(MODELS) * len(DATASETS)\n",
        "experiment_num = 0\n",
        "\n",
        "print(f\"Starting {total_experiments} experiments: {len(MODELS)} models x {len(DATASETS)} datasets\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for model_name in MODELS:\n",
        "    for dataset_name in DATASETS:\n",
        "        experiment_num += 1\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"EXPERIMENT {experiment_num}/{total_experiments}: {model_name} on {dataset_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        try:\n",
        "            # Get model type\n",
        "            model_type = MODEL_TYPES[model_name]\n",
        "            \n",
        "            # Load data and create loaders\n",
        "            train_loader, val_loader, dataset_info = get_loaders(\n",
        "                dataset_name=dataset_name,\n",
        "                patch_size=patch_size,\n",
        "                batch_size=batch_size,\n",
        "                val_split=val_split,\n",
        "                model_type=model_type\n",
        "            )\n",
        "            \n",
        "            # Create model with corresponding parameters\n",
        "            num_bands = dataset_info['num_bands']\n",
        "            num_classes = dataset_info['num_classes']\n",
        "            \n",
        "            model = create_model(model_name, num_bands, num_classes, patch_size)\n",
        "            print(f\"Model: {model_name}, Input: {num_bands} bands, {num_classes} classes\")\n",
        "            \n",
        "            # Train\n",
        "            history, best_val_acc = train(\n",
        "                model, train_loader, val_loader, \n",
        "                epochs=epochs, lr=learning_rate, device=device,\n",
        "                model_name=model_name, dataset_name=dataset_name\n",
        "            )\n",
        "            \n",
        "            # Save results\n",
        "            results[f\"{model_name}_{dataset_name}\"] = {\n",
        "                'model': model_name,\n",
        "                'dataset': dataset_name,\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'num_bands': num_bands,\n",
        "                'num_classes': num_classes,\n",
        "                'num_samples': len(train_loader.dataset) + len(val_loader.dataset)\n",
        "            }\n",
        "            \n",
        "            print(f\"Finished: {model_name} on {dataset_name} - Val Acc: {best_val_acc:.2f}%\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error in experiment {model_name} on {dataset_name}: {str(e)}\")\n",
        "            results[f\"{model_name}_{dataset_name}\"] = {\n",
        "                'model': model_name,\n",
        "                'dataset': dataset_name,\n",
        "                'error': str(e)\n",
        "            }\n",
        "        \n",
        "        print()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"ALL EXPERIMENTS FINISHED\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dataframe for results\n",
        "results_list = []\n",
        "for key, value in results.items():\n",
        "    if 'error' not in value:\n",
        "        results_list.append(value)\n",
        "\n",
        "if results_list:\n",
        "    df_results = pd.DataFrame(results_list)\n",
        "    \n",
        "    pivot_results = df_results.pivot(index='model', columns='dataset', values='best_val_acc')\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Validation Accuracy (%)\")\n",
        "    print(\"=\"*80)\n",
        "    print(pivot_results.round(2))\n",
        "    \n",
        "    # Save to CSV\n",
        "    df_results.to_csv('all_results.csv', index=False)\n",
        "    pivot_results.to_csv('results_pivot.csv')\n",
        "    print(f\"\\nResults saved to all_results.csv and results_pivot.csv\")\n",
        "    \n",
        "    # Find best results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BEST RESULTS:\")\n",
        "    print(\"=\"*80)\n",
        "    best_overall = df_results.loc[df_results['best_val_acc'].idxmax()]\n",
        "    print(f\"Best result: {best_overall['model']} on {best_overall['dataset']}: {best_overall['best_val_acc']:.2f}%\")\n",
        "    \n",
        "    # Average per model\n",
        "    print(\"\\nAverage accuracy per model:\")\n",
        "    print(df_results.groupby('model')['best_val_acc'].mean().round(2).sort_values(ascending=False))\n",
        "    \n",
        "    # Average per dataset\n",
        "    print(\"\\nAverage accuracy per dataset:\")\n",
        "    print(df_results.groupby('dataset')['best_val_acc'].mean().round(2).sort_values(ascending=False))\n",
        "else:\n",
        "    print(\"No results to display\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'results_list' not in locals():\n",
        "    results_list = []\n",
        "    for key, value in results.items():\n",
        "        if 'error' not in value:\n",
        "            results_list.append(value)\n",
        "\n",
        "if results_list:\n",
        "    df_results = pd.DataFrame(results_list)\n",
        "    \n",
        "    # Heatmap of results\n",
        "    pivot_results = df_results.pivot(index='model', columns='dataset', values='best_val_acc')\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(pivot_results.values, aspect='auto', cmap='YlOrRd')\n",
        "    plt.colorbar(label='Validation Accuracy (%)')\n",
        "    plt.xticks(range(len(pivot_results.columns)), pivot_results.columns, rotation=45)\n",
        "    plt.yticks(range(len(pivot_results.index)), pivot_results.index)\n",
        "    plt.xlabel('Dataset')\n",
        "    plt.ylabel('Model')\n",
        "    plt.title('Validation Accuracy per Model-Dataset Combination')\n",
        "    \n",
        "    # Add values to heatmap\n",
        "    for i in range(len(pivot_results.index)):\n",
        "        for j in range(len(pivot_results.columns)):\n",
        "            text = plt.text(j, i, f'{pivot_results.iloc[i, j]:.1f}',\n",
        "                           ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results_heatmap.png', dpi=150)\n",
        "    plt.show()\n",
        "    print(\"Heatmap saved to results_heatmap.png\")\n",
        "    \n",
        "    # Bar plot - średnie per model\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    mean_per_model = df_results.groupby('model')['best_val_acc'].mean().sort_values(ascending=False)\n",
        "    mean_per_model.plot(kind='bar', color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "    plt.ylabel('Average Validation Accuracy (%)')\n",
        "    plt.xlabel('Model')\n",
        "    plt.title('Average Performance per Model Across All Datasets')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('results_per_model.png', dpi=150)\n",
        "    plt.show()\n",
        "    print(\"Plot of results per model saved to results_per_model.png\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
