{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972d2a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importujemy wymagane biblioteki\n",
    "import os\n",
    "import urllib.request\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import csv\n",
    "\n",
    "# Parametry\n",
    "patch_size = 16\n",
    "\n",
    "# Funkcja do pobierania plików\n",
    "def download_file(url, filename):\n",
    "    if not os.path.exists(filename):\n",
    "        print(f\"Pobieranie {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Pobrano {filename}\")\n",
    "\n",
    "# Ładowanie danych\n",
    "def load_indian_pines():\n",
    "    data_url = \"https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat\"\n",
    "    label_url = \"https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\"\n",
    "    data_file = \"Indian_pines_corrected.mat\"\n",
    "    label_file = \"Indian_pines_gt.mat\"\n",
    "    download_file(data_url, data_file)\n",
    "    download_file(label_url, label_file)\n",
    "    data = sio.loadmat(data_file)[\"indian_pines_corrected\"]\n",
    "    labels = sio.loadmat(label_file)[\"indian_pines_gt\"]\n",
    "    return data, labels\n",
    "\n",
    "# Normalizacja\n",
    "def normalize(data):\n",
    "    h, w, b = data.shape\n",
    "    data = data.reshape(-1, b)\n",
    "    data = MinMaxScaler().fit_transform(data)\n",
    "    return data.reshape(h, w, b)\n",
    "\n",
    "# Padding\n",
    "def pad_with_zeros(data, margin):\n",
    "    return np.pad(data, ((margin, margin), (margin, margin), (0, 0)), mode='constant')\n",
    "\n",
    "# Dataset\n",
    "class HSI_Dataset(Dataset):\n",
    "    def __init__(self, patch_size=patch_size):\n",
    "        data, labels = load_indian_pines()\n",
    "        data = normalize(data)\n",
    "        margin = patch_size // 2\n",
    "        padded_data = pad_with_zeros(data, margin)\n",
    "\n",
    "        h, w, _ = data.shape\n",
    "        self.patches = []\n",
    "        self.targets = []\n",
    "\n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                label = labels[i, j]\n",
    "                if label == 0:\n",
    "                    continue\n",
    "                patch = padded_data[i:i+patch_size, j:j+patch_size, :]\n",
    "                self.patches.append(patch)\n",
    "                self.targets.append(label - 1)\n",
    "\n",
    "        self.patches = np.array(self.patches)\n",
    "        self.patches = np.transpose(self.patches, (0, 3, 1, 2))  # (N, B, H, W)\n",
    "        self.targets = np.array(self.targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.patches[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "\n",
    "# Loadery\n",
    "def get_loaders(batch_size=32, patch_size=patch_size, val_split=0.2):\n",
    "    dataset = HSI_Dataset(patch_size)\n",
    "    val_len = int(len(dataset) * val_split)\n",
    "    train_len = len(dataset) - val_len\n",
    "    train_set, val_set = random_split(dataset, [train_len, val_len])\n",
    "    return DataLoader(train_set, batch_size=batch_size, shuffle=True), DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "# CNN z diagramu\n",
    "class CNNFromDiagram(nn.Module):\n",
    "    def __init__(self, input_channels=200, num_classes=16, patch_size=16):\n",
    "        super(CNNFromDiagram, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=100, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=100, out_channels=100, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Oblicz rozmiar wyjścia po konwolucjach\n",
    "        dummy_input = torch.zeros(1, input_channels, patch_size, patch_size)\n",
    "        x = self.pool1(F.relu(self.conv1(dummy_input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        flatten_dim = x.view(1, -1).shape[1]  # <-- to oblicza prawidłowy rozmiar\n",
    "\n",
    "        self.fc1 = nn.Linear(flatten_dim, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Trenowanie\n",
    "def train(model, train_loader, val_loader, epochs=10, lr=0.001, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    with open(\"training_log.csv\", mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Validation Accuracy\"])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss, correct = 0, 0\n",
    "\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "            acc = 100.0 * correct / len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            val_correct = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_correct += (val_outputs.argmax(1) == val_labels).sum().item()\n",
    "            val_acc = 100.0 * val_correct / len(val_loader.dataset)\n",
    "\n",
    "            writer.writerow([epoch + 1, total_loss, acc, val_acc])\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%, Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "# Predykcja całej sceny\n",
    "def predict_whole_scene(model, patch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    data, labels = load_indian_pines()\n",
    "    data = normalize(data)\n",
    "    h, w, b = data.shape\n",
    "    margin = patch_size // 2\n",
    "    padded_data = pad_with_zeros(data, margin)\n",
    "    output = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    for i in range(h):\n",
    "        for j in range(w):\n",
    "            if labels[i, j] == 0:\n",
    "                continue\n",
    "            patch = padded_data[i:i+patch_size, j:j+patch_size, :]\n",
    "            patch = np.expand_dims(patch, axis=0)\n",
    "            patch = np.transpose(patch, (0, 3, 1, 2))  # (1, B, H, W)\n",
    "            patch = torch.tensor(patch, dtype=torch.float32).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = model(patch)\n",
    "                output[i, j] = pred.argmax(1).item() + 1  # +1 by pasowało do etykiet\n",
    "\n",
    "    return output, labels\n",
    "\n",
    "# Wizualizacja\n",
    "def visualize(pred_map, true_map):\n",
    "    class_names = [\n",
    "        \"Background\", \"Alfalfa\", \"Corn-notill\", \"Corn-mintill\", \"Corn\",\n",
    "        \"Grass-pasture\", \"Grass-trees\", \"Grass-pasture-mowed\", \"Hay-windrowed\",\n",
    "        \"Oats\", \"Soybean-notill\", \"Soybean-mintill\", \"Soybean-clean\",\n",
    "        \"Wheat\", \"Woods\", \"Buildings-Grass-Trees-Drives\", \"Stone-Steel-Towers\"\n",
    "    ]\n",
    "\n",
    "    cmap = ListedColormap([\n",
    "        (0, 0, 0), (0.6, 0, 0), (0, 0.6, 0), (0, 0, 0.6), (0.6, 0.6, 0),\n",
    "        (0.6, 0, 0.6), (0, 0.6, 0.6), (0.9, 0.3, 0.1), (0.1, 0.5, 0.9),\n",
    "        (0.7, 0.2, 0.7), (0.2, 0.7, 0.2), (0.9, 0.9, 0), (0.3, 0.3, 0.3),\n",
    "        (0.5, 0.2, 0.2), (0.2, 0.5, 0.2), (0.2, 0.2, 0.5), (1.0, 1.0, 1.0)\n",
    "    ])\n",
    "\n",
    "    norm = BoundaryNorm(boundaries=np.arange(17) - 0.5, ncolors=17)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    axs[0].imshow(true_map, cmap=cmap, norm=norm)\n",
    "    axs[0].set_title(\"Ground Truth\")\n",
    "    axs[0].axis(\"off\")\n",
    "\n",
    "    axs[1].imshow(pred_map, cmap=cmap, norm=norm)\n",
    "    axs[1].set_title(\"Predicted Map\")\n",
    "    axs[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    patches = [mpatches.Patch(color=cmap(i), label=f\"{i}: {class_names[i]}\")\n",
    "               for i in range(1, len(class_names))]\n",
    "    fig.legend(handles=patches, loc='center right', bbox_to_anchor=(1.15, 0.5), title=\"Classes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Główne wywołanie\n",
    "if __name__ == \"__main__\":\n",
    "    train_loader, val_loader = get_loaders(batch_size=100, patch_size=patch_size)\n",
    "    model = CNNFromDiagram(input_channels=200, num_classes=16, patch_size=patch_size)\n",
    "    train(model, train_loader, val_loader, epochs=100)\n",
    "    pred_map, true_map = predict_whole_scene(model, patch_size=patch_size)\n",
    "    visualize(pred_map, true_map)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
