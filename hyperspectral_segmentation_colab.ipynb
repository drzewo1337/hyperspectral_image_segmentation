{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Segmentacja Obraz√≥w Hiperspektralnych - Pe≈Çny Flow (TensorFlow/TPU)\n",
        "\n",
        "Ten notebook zawiera kompletny pipeline do segmentacji obraz√≥w hiperspektralnych u≈ºywajƒÖc **DBSCAN clustering** z **TensorFlow/Keras** (wsparcie dla TPU).\n",
        "\n",
        "## Instrukcja u≈ºycia w Google Colab:\n",
        "\n",
        "1. **W≈ÇƒÖcz TPU**: Runtime ‚Üí Change runtime type ‚Üí TPU (lub GPU je≈õli TPU niedostƒôpne)\n",
        "2. **Uruchom wszystkie kom√≥rki**: Runtime ‚Üí Run all\n",
        "3. **Parametry mo≈ºna zmieniƒá** w kom√≥rce z `TARGET_BANDS = [10, 20]`\n",
        "4. **‚ö† Oszczƒôdzanie RAM**: Notebook automatycznie ≈Çaduje datasety na ≈ºƒÖdanie i czy≈õci pamiƒôƒá\n",
        "\n",
        "## Flow:\n",
        "1. **Wczytanie danych** (5 dataset√≥w: Indian, PaviaU, PaviaC, KSC, Salinas)\n",
        "2. **Preprocessing** - redukcja wymiar√≥w przez filtr Gaussa (10/20/30 kana≈Ç√≥w)\n",
        "3. **Stworzenie zbioru testowego** - N split√≥w (Train: 3, Test: 1, Validation: 1)\n",
        "4. **Testy 3 modeli** z DBSCAN clustering do segmentacji\n",
        "5. **Walidacja i podsumowanie** wynik√≥w\n",
        "\n",
        "## Modele (TensorFlow/Keras):\n",
        "- InceptionHSINet (3D CNN)\n",
        "- SimpleHSINet (2D CNN)\n",
        "- CNNFromDiagram (2D CNN)\n",
        "\n",
        "## Segmentacja:\n",
        "U≈ºywa **DBSCAN clustering** - automatycznie znajduje liczbƒô segment√≥w bez znanych klas z g√≥ry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalacja pakiet√≥w\n",
        "%pip install tensorflow scikit-learn scipy matplotlib requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import bibliotek\n",
        "import os\n",
        "import urllib.request\n",
        "import ssl\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from scipy import ndimage\n",
        "from scipy.stats import mode\n",
        "import itertools\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Instalacja psutil je≈õli nie ma\n",
        "try:\n",
        "    import psutil\n",
        "except ImportError:\n",
        "    print(\"Instalowanie psutil...\")\n",
        "    import subprocess\n",
        "    subprocess.check_call(['pip', 'install', 'psutil'])\n",
        "    import psutil\n",
        "\n",
        "# Obs≈Çuga SSL\n",
        "try:\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "except:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    import requests\n",
        "    requests.packages.urllib3.disable_warnings()\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "# Sprawd≈∫ dostƒôpno≈õƒá TPU\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    print('Running on TPU ', tpu.master())\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "    print(\"‚úì TPU znaleziony - u≈ºywam TPU\")\n",
        "    USE_TPU = True\n",
        "except ValueError:\n",
        "    print(\"‚ö† TPU nie znaleziony - sprawdzam GPU...\")\n",
        "    try:\n",
        "        gpus = tf.config.list_physical_devices('GPU')\n",
        "        if gpus:\n",
        "            print(f\"‚úì GPU znaleziony: {len(gpus)} GPU(s)\")\n",
        "            USE_TPU = False\n",
        "        else:\n",
        "            print(\"‚ö† Brak GPU - u≈ºywam CPU\")\n",
        "            USE_TPU = False\n",
        "    except:\n",
        "        print(\"‚ö† U≈ºywam CPU\")\n",
        "        USE_TPU = False\n",
        "\n",
        "print(\"‚úì Biblioteki zaimportowane\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KROK 1: Definicje modeli i funkcji pomocniczych\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== MODELE TENSORFLOW/KERAS ==========\n",
        "\n",
        "def create_InceptionHSINet(input_shape, num_classes=16):\n",
        "    \"\"\"InceptionHSINet - 3D CNN dla TensorFlow\"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    # Entry\n",
        "    x = layers.Conv3D(8, 3, padding='same')(inputs)\n",
        "    x = layers.SpatialDropout3D(0.3)(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPool3D(2)(x)\n",
        "    \n",
        "    # Branch 1\n",
        "    x1 = layers.Conv3D(16, 1, padding='same')(x)\n",
        "    x1 = layers.SpatialDropout3D(0.3)(x1)\n",
        "    x1 = layers.ReLU()(x1)\n",
        "    x1 = layers.Conv3D(16, 3, padding='same')(x1)\n",
        "    x1 = layers.SpatialDropout3D(0.3)(x1)\n",
        "    x1 = layers.ReLU()(x1)\n",
        "    \n",
        "    # Branch 2\n",
        "    x2 = layers.Conv3D(16, 3, padding='same')(x)\n",
        "    x2 = layers.SpatialDropout3D(0.3)(x2)\n",
        "    x2 = layers.ReLU()(x2)\n",
        "    x2 = layers.Conv3D(16, 5, padding='same')(x2)\n",
        "    x2 = layers.SpatialDropout3D(0.3)(x2)\n",
        "    x2 = layers.ReLU()(x2)\n",
        "    \n",
        "    # Branch 3\n",
        "    x3 = layers.Conv3D(16, 5, padding='same')(x)\n",
        "    x3 = layers.SpatialDropout3D(0.3)(x3)\n",
        "    x3 = layers.ReLU()(x3)\n",
        "    x3 = layers.Conv3D(16, 3, padding='same')(x3)\n",
        "    x3 = layers.SpatialDropout3D(0.3)(x3)\n",
        "    x3 = layers.ReLU()(x3)\n",
        "    \n",
        "    # Concatenate\n",
        "    x = layers.Concatenate()([x1, x2, x3])\n",
        "    x = layers.GlobalAveragePooling3D()(x)\n",
        "    \n",
        "    # Feature extraction (bez klasyfikatora)\n",
        "    features = x\n",
        "    \n",
        "    # Classifier\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    outputs = layers.Dense(num_classes)(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    feature_model = Model(inputs=inputs, outputs=features)\n",
        "    \n",
        "    return model, feature_model\n",
        "\n",
        "\n",
        "def create_SimpleHSINet(input_shape, num_classes=16):\n",
        "    \"\"\"SimpleHSINet - 2D CNN dla TensorFlow\"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    x = layers.Conv2D(90, 1, padding='same')(inputs)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.Conv2D(270, 3, padding='same')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.SpatialDropout2D(0.3)(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    \n",
        "    # Feature extraction\n",
        "    features = layers.Dense(180, activation='relu')(x)\n",
        "    \n",
        "    # Classifier\n",
        "    x = layers.Dropout(0.3)(features)\n",
        "    outputs = layers.Dense(num_classes)(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    feature_model = Model(inputs=inputs, outputs=features)\n",
        "    \n",
        "    return model, feature_model\n",
        "\n",
        "\n",
        "def create_CNNFromDiagram(input_shape, num_classes=16):\n",
        "    \"\"\"CNNFromDiagram - 2D CNN dla TensorFlow\"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    x = layers.Conv2D(100, 3, padding='same')(inputs)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Conv2D(100, 3, padding='same')(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    \n",
        "    # Feature extraction\n",
        "    features = layers.Dense(84, activation='relu')(x)\n",
        "    \n",
        "    # Classifier\n",
        "    outputs = layers.Dense(num_classes)(features)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    feature_model = Model(inputs=inputs, outputs=features)\n",
        "    \n",
        "    return model, feature_model\n",
        "\n",
        "\n",
        "# Funkcja do tworzenia modeli\n",
        "def create_model(model_name, input_shape, num_classes):\n",
        "    \"\"\"Tworzy model i feature extractor\"\"\"\n",
        "    if model_name == 'InceptionHSINet':\n",
        "        return create_InceptionHSINet(input_shape, num_classes)\n",
        "    elif model_name == 'SimpleHSINet':\n",
        "        return create_SimpleHSINet(input_shape, num_classes)\n",
        "    elif model_name == 'CNNFromDiagram':\n",
        "        return create_CNNFromDiagram(input_shape, num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "MODELS = ['InceptionHSINet', 'SimpleHSINet', 'CNNFromDiagram']\n",
        "\n",
        "print(\"‚úì Modele TensorFlow/Keras zdefiniowane\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== FUNKCJE POMOCNICZE ==========\n",
        "\n",
        "DATASET_URLS = {\n",
        "    'Indian': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/6/67/Indian_pines_corrected.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat',\n",
        "    },\n",
        "    'PaviaU': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/e/ee/PaviaU.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/5/50/PaviaU_gt.mat',\n",
        "    },\n",
        "    'PaviaC': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/e/e3/Pavia.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/5/53/Pavia_gt.mat',\n",
        "    },\n",
        "    'KSC': {\n",
        "        'data': 'http://www.ehu.es/ccwintco/uploads/2/26/KSC.mat',\n",
        "        'gt':   'http://www.ehu.es/ccwintco/uploads/a/a6/KSC_gt.mat',\n",
        "    },\n",
        "    'Salinas': {\n",
        "        'data': 'https://www.ehu.eus/ccwintco/uploads/a/a3/Salinas_corrected.mat',\n",
        "        'gt':   'https://www.ehu.eus/ccwintco/uploads/f/fa/Salinas_gt.mat',\n",
        "    }\n",
        "}\n",
        "\n",
        "DATASET_KEYS = {\n",
        "    'Indian': {'data': ['indian_pines_corrected', 'Indian_pines_corrected'], 'gt': ['indian_pines_gt', 'Indian_pines_gt']},\n",
        "    'PaviaU': {'data': ['paviaU', 'PaviaU', 'pavia_u'], 'gt': ['paviaU_gt', 'PaviaU_gt', 'pavia_u_gt']},\n",
        "    'PaviaC': {'data': ['pavia', 'Pavia', 'paviaC', 'PaviaC'], 'gt': ['pavia_gt', 'Pavia_gt', 'paviaC_gt', 'PaviaC_gt']},\n",
        "    'KSC': {'data': ['KSC', 'ksc'], 'gt': ['KSC_gt', 'ksc_gt']},\n",
        "    'Salinas': {'data': ['salinas_corrected', 'Salinas_corrected', 'salinas'], 'gt': ['salinas_gt', 'Salinas_gt']}\n",
        "}\n",
        "\n",
        "DATASET_INFO = {\n",
        "    'Indian': {'num_classes': 16, 'num_bands': 200},\n",
        "    'PaviaU': {'num_classes': 9, 'num_bands': 103},\n",
        "    'PaviaC': {'num_classes': 9, 'num_bands': 102},\n",
        "    'KSC': {'num_classes': 13, 'num_bands': 176},\n",
        "    'Salinas': {'num_classes': 16, 'num_bands': 204}\n",
        "}\n",
        "\n",
        "def download_file(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"Pobieranie {filename}...\")\n",
        "        urllib.request.urlretrieve(url, filename)\n",
        "        print(f\"‚úì Pobrano {filename}\")\n",
        "\n",
        "def find_key_in_mat(mat_file, possible_keys):\n",
        "    if isinstance(possible_keys, str):\n",
        "        possible_keys = [possible_keys]\n",
        "    for key in possible_keys:\n",
        "        if key in mat_file:\n",
        "            return key\n",
        "    keys = [k for k in mat_file.keys() if not k.startswith('__')]\n",
        "    if keys:\n",
        "        return keys[0]\n",
        "    raise ValueError(f\"Nie znaleziono klucza w pliku .mat\")\n",
        "\n",
        "def load_data(dataset_name):\n",
        "    urls = DATASET_URLS[dataset_name]\n",
        "    keys = DATASET_KEYS[dataset_name]\n",
        "    data_file = f\"{dataset_name}_data.mat\"\n",
        "    gt_file = f\"{dataset_name}_gt.mat\"\n",
        "    download_file(urls['data'], data_file)\n",
        "    download_file(urls['gt'], gt_file)\n",
        "    mat_data = sio.loadmat(data_file)\n",
        "    mat_gt = sio.loadmat(gt_file)\n",
        "    data_key = find_key_in_mat(mat_data, keys['data'])\n",
        "    gt_key = find_key_in_mat(mat_gt, keys['gt'])\n",
        "    data = mat_data[data_key]\n",
        "    labels = mat_gt[gt_key]\n",
        "    print(f\"‚úì Za≈Çadowano {dataset_name}: shape={data.shape}, bands={data.shape[2]}\")\n",
        "    return data, labels\n",
        "\n",
        "def normalize(data):\n",
        "    h, w, b = data.shape\n",
        "    data = data.reshape(-1, b)\n",
        "    data = StandardScaler().fit_transform(data)\n",
        "    return data.reshape(h, w, b)\n",
        "\n",
        "def pad_with_zeros(data, margin):\n",
        "    return np.pad(data, ((margin, margin), (margin, margin), (0, 0)), mode='constant')\n",
        "\n",
        "print(\"‚úì Funkcje pomocnicze zdefiniowane\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parametry - ZOPTYMALIZOWANE DLA PAMIƒòCI\n",
        "TARGET_BANDS = [10, 20]  # Zmniejszono z [10, 20, 30] - mniej pamiƒôci\n",
        "PATCH_SIZE = 16\n",
        "BATCH_SIZE = 64  # Zmniejszono z 128 - mniej pamiƒôci na batch\n",
        "EPOCHS = 30  # Zmniejszono z 50 - szybsze treningi\n",
        "LR = 0.001\n",
        "N_SPLITS = 1  # Liczba split√≥w: 1=szybko (1 test), 3=≈õrednio, 5=pe≈Çne testy (d≈Çu≈ºej)\n",
        "\n",
        "# OPCJE OPTYMALIZACJI PAMIƒòCI\n",
        "LOAD_DATASETS_ON_DEMAND = True  # ≈Åaduj datasety tylko gdy potrzebne (oszczƒôdza RAM)\n",
        "CLEAR_MEMORY_BETWEEN_SPLITS = True  # Czy≈õƒá pamiƒôƒá miƒôdzy splitami\n",
        "PROCESS_ONE_DATASET_AT_TIME = True  # Przetwarzaj jeden dataset na raz\n",
        "\n",
        "DATASET_NAMES = ['Indian', 'PaviaU', 'PaviaC', 'KSC', 'Salinas']\n",
        "\n",
        "import gc  # Garbage collection\n",
        "\n",
        "# ≈Åadowanie wszystkich dataset√≥w\n",
        "print(\"=\" * 80)\n",
        "print(\"KROK 1: Wczytanie danych\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "datasets = {}\n",
        "for dataset_name in DATASET_NAMES:\n",
        "    print(f\"\\n≈Åadowanie {dataset_name}...\")\n",
        "    data, labels = load_data(dataset_name)\n",
        "    datasets[dataset_name] = {\n",
        "        'data': data,\n",
        "        'labels': labels,\n",
        "        'info': DATASET_INFO[dataset_name]\n",
        "    }\n",
        "\n",
        "print(f\"\\n‚úì Wczytano wszystkie {len(datasets)} dataset√≥w\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing - redukcja wymiar√≥w przez filtr Gaussa\n",
        "def gaussian_band_reduction(data, target_bands, sigma=1.0):\n",
        "    H, W, B = data.shape\n",
        "    if B <= target_bands:\n",
        "        return data\n",
        "    step = B / target_bands\n",
        "    indices = np.round(np.arange(0, B, step)).astype(int)\n",
        "    indices = indices[:target_bands]\n",
        "    selected_bands = data[:, :, indices]\n",
        "    filtered_bands = np.zeros_like(selected_bands)\n",
        "    for i in range(target_bands):\n",
        "        filtered_bands[:, :, i] = ndimage.gaussian_filter(selected_bands[:, :, i], sigma=sigma)\n",
        "    return filtered_bands\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"KROK 2: Preprocessing - redukcja wymiar√≥w (Gauss)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "preprocessed_data = {}\n",
        "for dataset_name, dataset_data in datasets.items():\n",
        "    data = dataset_data['data']\n",
        "    labels = dataset_data['labels']\n",
        "    info = dataset_data['info']\n",
        "    print(f\"\\nPreprocessing {dataset_name}...\")\n",
        "    data_normalized = normalize(data)\n",
        "    original_bands = data.shape[2]\n",
        "    \n",
        "    for target_bands in TARGET_BANDS:\n",
        "        if target_bands >= original_bands:\n",
        "            data_reduced = data_normalized\n",
        "            print(f\"  {target_bands} kana≈Ç√≥w: {original_bands} (oryginalne)\")\n",
        "        else:\n",
        "            data_reduced = gaussian_band_reduction(data_normalized, target_bands, sigma=1.0)\n",
        "            print(f\"  {target_bands} kana≈Ç√≥w: {original_bands} -> {target_bands} (Gauss)\")\n",
        "        \n",
        "        key = (dataset_name, target_bands)\n",
        "        preprocessed_data[key] = {\n",
        "            'data': data_reduced,\n",
        "            'labels': labels,\n",
        "            'info': {**info, 'num_bands': data_reduced.shape[2], 'original_bands': original_bands}\n",
        "        }\n",
        "\n",
        "print(f\"\\n‚úì Preprocessing zako≈Ñczony\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generowanie podzia≈Ç√≥w dataset√≥w\n",
        "print(\"=\" * 80)\n",
        "print(\"KROK 3: Stworzenie zbioru testowego\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Generowanie {N_SPLITS} split√≥w (mo≈ºesz zmieniƒá N_SPLITS w parametrach)\")\n",
        "\n",
        "train_combinations = list(itertools.combinations(DATASET_NAMES, 3))\n",
        "splits = []\n",
        "\n",
        "for split_id, train_datasets in enumerate(train_combinations, 1):\n",
        "    train_list = list(train_datasets)\n",
        "    remaining = [d for d in DATASET_NAMES if d not in train_list]\n",
        "    test_combinations = list(itertools.permutations(remaining, 2))\n",
        "    \n",
        "    for test_idx, (test_dataset, validation_dataset) in enumerate(test_combinations):\n",
        "        split = {\n",
        "            'split_id': split_id * 10 + test_idx + 1,\n",
        "            'train_datasets': train_list,\n",
        "            'test_dataset': test_dataset,\n",
        "            'validation_dataset': validation_dataset\n",
        "        }\n",
        "        splits.append(split)\n",
        "\n",
        "# Wybierz N_SPLITS pierwszych\n",
        "if len(splits) > N_SPLITS:\n",
        "    selected_splits = []\n",
        "    seen_train_combos = set()\n",
        "    for split in splits:\n",
        "        train_key = tuple(sorted(split['train_datasets']))\n",
        "        if train_key not in seen_train_combos or len(selected_splits) < N_SPLITS:\n",
        "            selected_splits.append(split)\n",
        "            seen_train_combos.add(train_key)\n",
        "            if len(selected_splits) >= N_SPLITS:\n",
        "                break\n",
        "    splits = selected_splits[:N_SPLITS]\n",
        "\n",
        "print(f\"\\nWygenerowano {len(splits)} podzia≈Ç√≥w:\")\n",
        "for split in splits:\n",
        "    print(f\"  Split {split['split_id']}: Train={'+'.join(split['train_datasets'])}, Test={split['test_dataset']}, Val={split['validation_dataset']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KROK 3: Funkcje treningu i clusteringu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funkcja do przygotowania danych dla TensorFlow\n",
        "def prepare_dataset(data_dict, dataset_names, patch_size=16, model_type='2d'):\n",
        "    \"\"\"Przygotowuje dane jako numpy arrays dla TensorFlow\"\"\"\n",
        "    patches_list = []\n",
        "    targets_list = []\n",
        "    all_num_classes = set()\n",
        "    \n",
        "    target_bands_list = [data_dict[k]['data'].shape[2] for k in data_dict.keys() if k[0] in dataset_names]\n",
        "    if not target_bands_list:\n",
        "        raise ValueError(f\"Brak danych dla dataset√≥w: {dataset_names}\")\n",
        "    \n",
        "    target_bands = target_bands_list[0]\n",
        "    \n",
        "    for dataset_name in dataset_names:\n",
        "        dataset_keys = [k for k in data_dict.keys() if k[0] == dataset_name and data_dict[k]['data'].shape[2] == target_bands]\n",
        "        if not dataset_keys:\n",
        "            dataset_keys = [k for k in data_dict.keys() if k[0] == dataset_name]\n",
        "            if not dataset_keys:\n",
        "                continue\n",
        "        \n",
        "        key = dataset_keys[0]\n",
        "        data = data_dict[key]['data']\n",
        "        labels = data_dict[key]['labels']\n",
        "        info = data_dict[key]['info']\n",
        "        all_num_classes.add(info['num_classes'])\n",
        "        \n",
        "        margin = patch_size // 2\n",
        "        padded_data = pad_with_zeros(data, margin)\n",
        "        \n",
        "        h, w, _ = data.shape\n",
        "        for i in range(h):\n",
        "            for j in range(w):\n",
        "                label = labels[i, j]\n",
        "                if label == 0:\n",
        "                    continue\n",
        "                patch = padded_data[i:i+patch_size, j:j+patch_size, :]\n",
        "                patches_list.append(patch)\n",
        "                targets_list.append(label - 1)\n",
        "    \n",
        "    if len(patches_list) == 0:\n",
        "        raise ValueError(f\"Brak danych dla dataset√≥w: {dataset_names}\")\n",
        "    \n",
        "    patches = np.array(patches_list)\n",
        "    targets = np.array(targets_list)\n",
        "    \n",
        "    if len(patches.shape) == 4:\n",
        "        current_bands = patches.shape[-1]\n",
        "        if current_bands != target_bands:\n",
        "            if current_bands < target_bands:\n",
        "                padding = np.zeros((patches.shape[0], patches.shape[1], patches.shape[2], target_bands - current_bands))\n",
        "                patches = np.concatenate([patches, padding], axis=-1)\n",
        "            else:\n",
        "                patches = patches[:, :, :, :target_bands]\n",
        "    \n",
        "    if model_type == '3d':\n",
        "        # Conv3D: (N, H, W, B) -> (N, H, W, B, 1) dla TensorFlow\n",
        "        patches = np.expand_dims(patches, axis=-1)  # (N, H, W, B, 1)\n",
        "    else:\n",
        "        # Conv2D: (N, H, W, B) -> (N, H, W, B) - ju≈º OK\n",
        "        patches = patches\n",
        "    \n",
        "    num_bands = patches.shape[-2] if model_type == '3d' else patches.shape[-1]\n",
        "    num_classes = max(all_num_classes) if all_num_classes else 16\n",
        "    \n",
        "    print(f\"Dataset {dataset_names}: {len(patches)} samples, bands={num_bands}, classes={num_classes}\")\n",
        "    \n",
        "    return patches, targets, num_bands, num_classes\n",
        "\n",
        "print(\"‚úì Funkcje przygotowania danych zdefiniowane\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funkcja treningu TensorFlow\n",
        "def train_model(model, x_train, y_train, x_val, y_val, epochs=50, lr=0.001, batch_size=128, model_name=\"model\"):\n",
        "    \"\"\"Trenowanie modelu TensorFlow/Keras\"\"\"\n",
        "    \n",
        "    # Kompilacja modelu\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n",
        "        keras.callbacks.ModelCheckpoint(\n",
        "            f'best_model_{model_name}.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=0\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Trening\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1 if epochs <= 20 else 2\n",
        "    )\n",
        "    \n",
        "    best_val_acc = max(history.history['val_accuracy']) * 100\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"‚úì Funkcja treningu TensorFlow zdefiniowana\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funkcje clusteringu dla TensorFlow\n",
        "def extract_features_for_image(feature_model, data, labels, patch_size=16, model_type='2d', batch_size=64):\n",
        "    \"\"\"Ekstrahuje embeddingi u≈ºywajƒÖc TensorFlow model\"\"\"\n",
        "    margin = patch_size // 2\n",
        "    padded_data = pad_with_zeros(data, margin)\n",
        "    \n",
        "    h, w, _ = data.shape\n",
        "    patches = []\n",
        "    pixel_coords = []\n",
        "    \n",
        "    for i in range(h):\n",
        "        for j in range(w):\n",
        "            if labels[i, j] > 0:\n",
        "                patch = padded_data[i:i+patch_size, j:j+patch_size, :]\n",
        "                patches.append(patch)\n",
        "                pixel_coords.append((i, j))\n",
        "    \n",
        "    if len(patches) == 0:\n",
        "        raise ValueError(\"Brak pikseli z danymi\")\n",
        "    \n",
        "    patches = np.array(patches)\n",
        "    \n",
        "    if model_type == '3d':\n",
        "        # Conv3D: (N, H, W, B) -> (N, H, W, B, 1) dla TensorFlow\n",
        "        patches = np.expand_dims(patches, axis=-1)\n",
        "    else:\n",
        "        # Conv2D: (N, H, W, B) - ju≈º OK\n",
        "        pass\n",
        "    \n",
        "    # Ekstrahuj embeddingi w batchach\n",
        "    embeddings_list = []\n",
        "    for i in range(0, len(patches), batch_size):\n",
        "        batch = patches[i:i+batch_size]\n",
        "        batch_embeddings = feature_model.predict(batch, verbose=0)\n",
        "        embeddings_list.append(batch_embeddings)\n",
        "    \n",
        "    embeddings = np.concatenate(embeddings_list, axis=0)\n",
        "    \n",
        "    feature_dim = embeddings.shape[1]\n",
        "    embedding_map = np.zeros((h, w, feature_dim))\n",
        "    \n",
        "    for idx, (i, j) in enumerate(pixel_coords):\n",
        "        embedding_map[i, j] = embeddings[idx]\n",
        "    \n",
        "    return embedding_map, pixel_coords\n",
        "\n",
        "def segment_with_dbscan(feature_model, data, labels, patch_size=16, model_type='2d', eps=0.5, min_samples=5, batch_size=64):\n",
        "    print(f\"Segmentacja DBSCAN (eps={eps}, min_samples={min_samples})...\")\n",
        "    \n",
        "    embedding_map, pixel_coords = extract_features_for_image(feature_model, data, labels, patch_size, model_type, batch_size)\n",
        "    \n",
        "    embeddings_list = []\n",
        "    coords_list = []\n",
        "    for i, j in pixel_coords:\n",
        "        embeddings_list.append(embedding_map[i, j])\n",
        "        coords_list.append((i, j))\n",
        "    \n",
        "    embeddings_array = np.array(embeddings_list)\n",
        "    scaler = StandardScaler()\n",
        "    embeddings_normalized = scaler.fit_transform(embeddings_array)\n",
        "    \n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
        "    cluster_labels = dbscan.fit_predict(embeddings_normalized)\n",
        "    \n",
        "    h, w = data.shape[:2]\n",
        "    segmentation_map = np.zeros((h, w), dtype=np.int32)\n",
        "    \n",
        "    for idx, (i, j) in enumerate(coords_list):\n",
        "        cluster_id = cluster_labels[idx]\n",
        "        if cluster_id >= 0:\n",
        "            segmentation_map[i, j] = cluster_id + 1\n",
        "        else:\n",
        "            segmentation_map[i, j] = 0\n",
        "    \n",
        "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "    n_outliers = np.sum(cluster_labels == -1)\n",
        "    print(f\"  Znaleziono {n_clusters} klastr√≥w, {n_outliers} outliers\")\n",
        "    \n",
        "    return segmentation_map, n_clusters\n",
        "\n",
        "def evaluate_clustering(segmentation_map, ground_truth):\n",
        "    mask = ground_truth > 0\n",
        "    if np.sum(mask) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    segments = segmentation_map[mask]\n",
        "    classes = ground_truth[mask]\n",
        "    \n",
        "    unique_segments = np.unique(segments)\n",
        "    unique_segments = unique_segments[unique_segments > 0]\n",
        "    \n",
        "    if len(unique_segments) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    cluster_to_class = {}\n",
        "    for seg in unique_segments:\n",
        "        seg_mask = segments == seg\n",
        "        if np.sum(seg_mask) > 0:\n",
        "            most_common_class = mode(classes[seg_mask], keepdims=True)[0][0]\n",
        "            cluster_to_class[seg] = most_common_class\n",
        "    \n",
        "    mapped_segments = np.zeros_like(segments)\n",
        "    for seg, cls in cluster_to_class.items():\n",
        "        mapped_segments[segments == seg] = cls\n",
        "    \n",
        "    correct = np.sum(mapped_segments == classes)\n",
        "    total = len(classes)\n",
        "    \n",
        "    return 100.0 * correct / total if total > 0 else 0.0\n",
        "\n",
        "print(\"‚úì Funkcje clusteringu TensorFlow zdefiniowane\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KROK 4: Trening i testowanie modeli\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# G≈Ç√≥wna pƒôtla treningu i testowania TensorFlow\n",
        "print(f\"Device: {'TPU' if USE_TPU else 'GPU/CPU'}\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for target_bands in TARGET_BANDS:\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"# Testowanie dla {target_bands} kana≈Ç√≥w\")\n",
        "    print(f\"{'#'*80}\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for split in splits:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Split {split['split_id']}:\")\n",
        "        print(f\"  Train: {', '.join(split['train_datasets'])}\")\n",
        "        print(f\"  Test: {split['test_dataset']}\")\n",
        "        print(f\"  Validation: {split['validation_dataset']}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        # Za≈Çaduj dane na ≈ºƒÖdanie\n",
        "        train_data_dict = {}\n",
        "        for name in split['train_datasets']:\n",
        "            key = (name, target_bands)\n",
        "            if LOAD_DATASETS_ON_DEMAND:\n",
        "                if key not in preprocessed_data:\n",
        "                    # Za≈Çaduj i preprocessuj\n",
        "                    preprocessed_data[key] = load_and_preprocess_dataset(name, target_bands)\n",
        "                train_data_dict[key] = preprocessed_data[key]\n",
        "            else:\n",
        "                if key in preprocessed_data:\n",
        "                    train_data_dict[key] = preprocessed_data[key]\n",
        "        \n",
        "        if not train_data_dict:\n",
        "            print(f\"  ‚ö† Brak danych dla train datasets\")\n",
        "            continue\n",
        "        \n",
        "        for model_name in MODELS:\n",
        "            print(f\"\\n  Model: {model_name}\")\n",
        "            model_type = '3d' if model_name == 'InceptionHSINet' else '2d'\n",
        "            \n",
        "            try:\n",
        "                train_keys_filtered = {k: v for k, v in train_data_dict.items() if k[1] == target_bands}\n",
        "                if not train_keys_filtered:\n",
        "                    continue\n",
        "                \n",
        "                # Przygotuj dane\n",
        "                x_train_full, y_train_full, num_bands, num_classes = prepare_dataset(\n",
        "                    train_keys_filtered, split['train_datasets'], patch_size=PATCH_SIZE, model_type=model_type\n",
        "                )\n",
        "                \n",
        "                # Podzia≈Ç na train/val\n",
        "                n_train = int(len(x_train_full) * 0.8)\n",
        "                indices = np.random.permutation(len(x_train_full))\n",
        "                train_indices = indices[:n_train]\n",
        "                val_indices = indices[n_train:]\n",
        "                \n",
        "                x_train = x_train_full[train_indices]\n",
        "                y_train = y_train_full[train_indices]\n",
        "                x_val = x_train_full[val_indices]\n",
        "                y_val = y_train_full[val_indices]\n",
        "                \n",
        "                # Okre≈õl input_shape\n",
        "                if model_type == '3d':\n",
        "                    input_shape = (PATCH_SIZE, PATCH_SIZE, num_bands, 1)\n",
        "                else:\n",
        "                    input_shape = (PATCH_SIZE, PATCH_SIZE, num_bands)\n",
        "                \n",
        "                # Utw√≥rz model w strategii TPU je≈õli dostƒôpne\n",
        "                if USE_TPU:\n",
        "                    with strategy.scope():\n",
        "                        model, feature_model = create_model(model_name, input_shape, num_classes)\n",
        "                else:\n",
        "                    model, feature_model = create_model(model_name, input_shape, num_classes)\n",
        "                \n",
        "                print(f\"    Trenowanie... (bands={num_bands}, classes={num_classes})\")\n",
        "                trained_model = train_model(model, x_train, y_train, x_val, y_val, epochs=EPOCHS, lr=LR, batch_size=BATCH_SIZE, model_name=f\"{model_name}_{split['split_id']}\")\n",
        "                \n",
        "                # Za≈Çaduj najlepszy model\n",
        "                try:\n",
        "                    trained_model.load_weights(f'best_model_{model_name}_{split[\"split_id\"]}.h5')\n",
        "                except:\n",
        "                    pass\n",
        "                \n",
        "                # Skopiuj wagi z trained_model do feature_model\n",
        "                # Znajd≈∫ warstwƒô przed klasyfikatorem (features)\n",
        "                for i, layer in enumerate(trained_model.layers):\n",
        "                    if i < len(feature_model.layers):\n",
        "                        try:\n",
        "                            feature_model.layers[i].set_weights(layer.get_weights())\n",
        "                        except:\n",
        "                            pass\n",
        "                \n",
        "                # Test na test dataset - DBSCAN (≈Çadowanie na ≈ºƒÖdanie)\n",
        "                test_key = (split['test_dataset'], target_bands)\n",
        "                test_acc = 0.0\n",
        "                test_n_clusters = 0\n",
        "                test_n_samples = 0\n",
        "                \n",
        "                if LOAD_DATASETS_ON_DEMAND:\n",
        "                    if test_key not in preprocessed_data:\n",
        "                        preprocessed_data[test_key] = load_and_preprocess_dataset(split['test_dataset'], target_bands)\n",
        "                \n",
        "                if test_key in preprocessed_data:\n",
        "                    test_data = preprocessed_data[test_key]['data']\n",
        "                    test_labels = preprocessed_data[test_key]['labels']\n",
        "                    segmentation_map, n_clusters = segment_with_dbscan(feature_model, test_data, test_labels, patch_size=PATCH_SIZE, model_type=model_type, eps=0.5, min_samples=5, batch_size=BATCH_SIZE)\n",
        "                    test_acc = evaluate_clustering(segmentation_map, test_labels)\n",
        "                    test_n_clusters = n_clusters\n",
        "                    test_n_samples = np.sum(test_labels > 0)\n",
        "                    print(f\"    Test DBSCAN: accuracy={test_acc:.2f}%, clusters={n_clusters}\")\n",
        "                    # Wyczy≈õƒá dane testowe z pamiƒôci je≈õli nie sƒÖ potrzebne\n",
        "                    if CLEAR_MEMORY_BETWEEN_SPLITS and PROCESS_ONE_DATASET_AT_TIME:\n",
        "                        del test_data, test_labels, segmentation_map\n",
        "                \n",
        "                # Test na validation dataset - DBSCAN (≈Çadowanie na ≈ºƒÖdanie)\n",
        "                validation_dataset_name = split['validation_dataset']\n",
        "                final_test_key = (validation_dataset_name, target_bands)\n",
        "                final_test_acc = 0.0\n",
        "                final_test_n_clusters = 0\n",
        "                final_test_n_samples = 0\n",
        "                \n",
        "                if LOAD_DATASETS_ON_DEMAND:\n",
        "                    if final_test_key not in preprocessed_data:\n",
        "                        preprocessed_data[final_test_key] = load_and_preprocess_dataset(validation_dataset_name, target_bands)\n",
        "                \n",
        "                if final_test_key in preprocessed_data:\n",
        "                    validation_data = preprocessed_data[final_test_key]['data']\n",
        "                    validation_labels = preprocessed_data[final_test_key]['labels']\n",
        "                    segmentation_map, n_clusters = segment_with_dbscan(feature_model, validation_data, validation_labels, patch_size=PATCH_SIZE, model_type=model_type, eps=0.5, min_samples=5, batch_size=BATCH_SIZE)\n",
        "                    final_test_acc = evaluate_clustering(segmentation_map, validation_labels)\n",
        "                    final_test_n_clusters = n_clusters\n",
        "                    final_test_n_samples = np.sum(validation_labels > 0)\n",
        "                    print(f\"    Validation DBSCAN: accuracy={final_test_acc:.2f}%, clusters={n_clusters}\")\n",
        "                    # Wyczy≈õƒá dane walidacyjne z pamiƒôci je≈õli nie sƒÖ potrzebne\n",
        "                    if CLEAR_MEMORY_BETWEEN_SPLITS and PROCESS_ONE_DATASET_AT_TIME:\n",
        "                        del validation_data, validation_labels, segmentation_map\n",
        "                \n",
        "                result = {\n",
        "                    'split_id': split['split_id'],\n",
        "                    'model_name': model_name,\n",
        "                    'target_bands': target_bands,\n",
        "                    'train_datasets': split['train_datasets'],\n",
        "                    'test_dataset': split['test_dataset'],\n",
        "                    'validation_dataset': validation_dataset_name,\n",
        "                    'test_accuracy': test_acc,\n",
        "                    'test_n_clusters': test_n_clusters,\n",
        "                    'validation_accuracy': final_test_acc,\n",
        "                    'validation_n_clusters': final_test_n_clusters,\n",
        "                    'test_n_samples': test_n_samples,\n",
        "                    'validation_n_samples': final_test_n_samples\n",
        "                }\n",
        "                results.append(result)\n",
        "                \n",
        "                # Wyczy≈õƒá pamiƒôƒá\n",
        "                del model, feature_model, trained_model, x_train, y_train, x_val, y_val, x_train_full, y_train_full\n",
        "                tf.keras.backend.clear_session()\n",
        "                gc.collect()\n",
        "                \n",
        "                # Je≈õli przetwarzamy jeden dataset na raz, wyczy≈õƒá preprocessed_data\n",
        "                if CLEAR_MEMORY_BETWEEN_SPLITS and PROCESS_ONE_DATASET_AT_TIME:\n",
        "                    # Usu≈Ñ dane treningowe z pamiƒôci\n",
        "                    for key in list(train_data_dict.keys()):\n",
        "                        if key in preprocessed_data:\n",
        "                            del preprocessed_data[key]['data']\n",
        "                            preprocessed_data[key]['data'] = None  # Zachowaj strukturƒô\n",
        "                    gc.collect()\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    ‚úó B≈ÇƒÖd: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                # Wyczy≈õƒá pamiƒôƒá nawet przy b≈Çƒôdzie\n",
        "                tf.keras.backend.clear_session()\n",
        "                gc.collect()\n",
        "                continue\n",
        "        \n",
        "        # Wyczy≈õƒá pamiƒôƒá miƒôdzy splitami\n",
        "        if CLEAR_MEMORY_BETWEEN_SPLITS:\n",
        "            print(f\"  üßπ Czyszczenie pamiƒôci po split {split['split_id']}...\")\n",
        "            tf.keras.backend.clear_session()\n",
        "            gc.collect()\n",
        "    \n",
        "    all_results[target_bands] = results\n",
        "    print(f\"\\n‚úì Zapisano wyniki dla {target_bands} kana≈Ç√≥w\")\n",
        "\n",
        "print(\"\\n‚úì Wszystkie testy zako≈Ñczone\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Podsumowanie wynik√≥w\n",
        "print(\"=\" * 80)\n",
        "print(\"PODSUMOWANIE WYNIK√ìW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for target_bands, results in all_results.items():\n",
        "    print(f\"\\n{target_bands} kana≈Ç√≥w:\")\n",
        "    for result in results:\n",
        "        print(f\"  {result['model_name']} - Split {result['split_id']}:\")\n",
        "        print(f\"    Test: {result['test_accuracy']:.2f}% ({result['test_n_clusters']} clusters)\")\n",
        "        print(f\"    Validation: {result['validation_accuracy']:.2f}% ({result['validation_n_clusters']} clusters)\")\n",
        "\n",
        "# Znajd≈∫ najlepszy model\n",
        "best_model = None\n",
        "best_score = -1\n",
        "for target_bands, results in all_results.items():\n",
        "    for result in results:\n",
        "        avg_score = (result['test_accuracy'] + result['validation_accuracy']) / 2\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_model = result\n",
        "\n",
        "if best_model:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"NAJLEPSZY MODEL:\")\n",
        "    print(f\"  Model: {best_model['model_name']}\")\n",
        "    print(f\"  Bands: {best_model['target_bands']}\")\n",
        "    print(f\"  Split: {best_model['split_id']}\")\n",
        "    print(f\"  ≈örednia accuracy: {best_score:.2f}%\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "# Zapis wynik√≥w\n",
        "print(\"\\nZapisujƒô wyniki do pliku...\")\n",
        "with open('results.json', 'w') as f:\n",
        "    json.dump(all_results, f, indent=2, default=str)\n",
        "print(\"‚úì Wyniki zapisane do results.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KROK 6: Wizualizacja wynik√≥w (opcjonalne)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wizualizacja wynik√≥w segmentacji (przyk≈Çad dla pierwszego wyniku)\n",
        "if all_results:\n",
        "    target_bands = list(all_results.keys())[0]\n",
        "    result = all_results[target_bands][0] if all_results[target_bands] else None\n",
        "    \n",
        "    if result:\n",
        "        # Za≈Çaduj model i dane\n",
        "        model_name = result['model_name']\n",
        "        model_class = MODELS[model_name]\n",
        "        model_type = '3d' if model_name == 'InceptionHSINet' else '2d'\n",
        "        \n",
        "        # Tutaj mo≈ºesz dodaƒá kod do wizualizacji\n",
        "        # (wymaga ponownego treningu lub zapisania modelu)\n",
        "        print(f\"Wizualizacja dla {model_name} - wymaga ponownego treningu lub zapisanego modelu\")\n",
        "        print(\"Mo≈ºesz dodaƒá kod wizualizacji tutaj\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
